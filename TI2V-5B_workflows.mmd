%% TI2V-5B Workflows (Software + Algorithmic)
%% Source: wan/textimage2video.py (WanTI2V), wan/modules/vae2_2.py (Wan2_2_VAE), wan/modules/model.py (WanModel),
%%         wan/modules/t5.py (T5EncoderModel), wan/utils/fm_solvers(_unipc).py (Schedulers), generate.py


%% =========================
%% SOFTWARE WORKFLOW (Flow)
%% =========================
flowchart TD
    CLI[User CLI\npython generate.py --task ti2v-5B ...] --> ParseArgs[Parse args in generate.py]
    ParseArgs -->|task=ti2v-5B| CreatePipeline[Create WanTI2V(config, ckpt, device, ...)]

    subgraph Init[Pipeline Initialization]
        CreatePipeline --> LoadT5[Load T5 encoder\n(checkpoint+tokenizer)]
        CreatePipeline --> LoadVAE[Load Wan2_2_VAE\n(ckpt) -> device]
        CreatePipeline --> LoadDiT[Load WanModel.from_pretrained\n(5B dense DiT)]
        LoadDiT --> ConfModel[Configure model:\n- eval(), no grad\n- optional: sequence parallel\n- optional: FSDP (DiT)\n- optional: convert dtype (bf16)\n- move to device if needed]
    end

    ParseArgs --> Branch{Image provided?}
    Branch -->|Yes| I2VPath[Call WanTI2V.generate(..., img=Image)]
    Branch -->|No|  T2VPath[Call WanTI2V.generate(..., img=None)]

    I2VPath --> PreI2V[Preprocess image:\nresize->center-crop->normalize\nto tensor (B,C,T,H,W)]
    T2VPath --> PreT2V[Compute target latent shape from\nsize, frame_num, vae_stride, patch_size]

    PreI2V --> EncodeImg[vae.encode -> latent z]
    PreT2V --> InitNoise[Init random noise latent(s)]

    subgraph TextEnc[Text Encoding]
        LoadT5 -->|move to device if needed| T5Run[Encode prompt and neg-prompt\n-> context, context_null]
        T5Run -->|offload T5 back to CPU if enabled| T5Offload[T5 offload (optional)]
    end

    subgraph Sampling[Sampling Setup]
        PreI2V --> SeqLen[Compute seq_len for patches / SP]
        PreT2V --> SeqLen
        SeqLen --> Sched[Create Scheduler:\nUniPC or DPM++\nset timesteps / sigmas]
    end

    subgraph Loop[Iterative Denoising Loop]
        Sched --> ForEachT[for t in timesteps]
        ForEachT --> BuildTS[Build timestep tensor\n(mask-based tiling across grid)]
        BuildTS --> ForwardCond[DiT forward (cond):\nmodel(latent, t, context, seq_len)]
        ForwardCond --> ForwardUncond[DiT forward (uncond):\nmodel(latent, t, context_null, seq_len)]
        ForwardUncond --> CFG[Classifier-Free Guidance:\nnoise = uncond + gs*(cond-uncond)]
        CFG --> StepUpdate[scheduler.step(noise, t, latent)\n-> new latent]
        StepUpdate -->|I2V only| BlendMask[Blend with image-conditioned regions:\nlatent = (1-mask)*z + mask*latent]
        BlendMask --> NextIter[Next timestep]
        StepUpdate --> NextIter
    end

    NextIter --> DoneLoop[Loop complete]
    DoneLoop --> Decode[vae.decode(latent(s)) -> video tensor]
    Decode --> Save[Save MP4 with fps\n(save_video in utils)]
    Save --> Output[Return / write final video]

    %% Optional VRAM optimization
    ConfModel -. optional .-> OffloadDiT[Move DiT CPU/GPU between steps]
    OffloadDiT -. reduces VRAM .-> Loop


%% ============================
%% ALGORITHMIC WORKFLOW (Flow)
%% ============================
flowchart LR
    subgraph Inputs[Inputs]
        P[Text prompt]:::i -->|tokenize| T5[T5-XXL Encoder]:::op
        N[Negative prompt]:::i -->|tokenize| T5
        IMG[Optional reference image]:::i
        H[Size (W,H), frame_num, steps, guide_scale, seed]:::i
    end

    T5 --> C[context embeddings]:::d
    T5 --> Cn[negative context]:::d

    subgraph Latents[Latent Preparation]
        direction TB
        IMG --> PreI[Resize->CenterCrop->Normalize]:::op
        PreI --> Zenc[Encode via Wan2_2_VAE]:::op --> Z[z (latent video grid)]:::d
        H --> Shape[Compute latent target shape:\nF, H', W' from vae_stride & patch_size]:::op
        Shape --> Noise[Init Gaussian noise in latent space]:::d
        Z --> MaskBuild[Build spatial-temporal masks]:::op
        Noise --> L0[Initial latent]:::d
        Z -. if IMG absent, skip .-> MaskBuild
    end

    subgraph SchedSP[Scheduling & Parallel]
        direction TB
        H --> Sched2[Init Flow Scheduler:\nUniPC or DPM++]:::op
        Sched2 --> TS[timesteps / sigmas]:::d
        H --> SP[Optional: Sequence Parallel (Ulysses)]:::op
        H --> FSDP[Optional: FSDP sharding (DiT)]:::op
    end

    subgraph Iterate[Iterative Denoising]
        direction TB
        TS --> Tstep[For t in timesteps]:::ctl
        Tstep --> Tgrid[Tile timestep across grid\n(mask-aware)]:::op
        L0 --> Cond[DiT forward (cond): f(L, t, C)]:::op
        L0 --> Uncond[DiT forward (uncond): f(L, t, Cn)]:::op
        Cond --> CFG2[Classifier-Free Guidance:\nε = ε_u + s*(ε_c - ε_u)]:::op
        Uncond --> CFG2
        CFG2 --> Update[scheduler.step(ε, t, L)]:::op --> L1[Updated latent]:::d
        Z -->|I2V only| Blend[(1-mask)*Z + mask*L1]:::op --> L1b[Blended latent]:::d
        L1 --> Next
        L1b --> Next
        Next -->|replace L| L0
    end

    subgraph DecodeOut[Decode & Output]
        direction TB
        L0 --> Dec[Decode via Wan2_2_VAE]:::op --> Vid[Video frames tensor]:::d
        Vid --> MP4[Write video (fps)]:::op --> OUT[Final MP4]
    end

    classDef i fill:#eef,stroke:#77a,color:#000;
    classDef op fill:#efe,stroke:#7a7,color:#000;
    classDef d fill:#fee,stroke:#a77,color:#000;
    classDef ctl fill:#ffd,stroke:#aa7,color:#000;

